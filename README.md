üéØ Descripci√≥n General:

Desarrollar un sistema interactivo de ‚Äúdrag & drop‚Äù (arrastrar y soltar) utilizando Python, implementando el seguimiento de manos con la biblioteca OpenCV y MediaPipe. El sistema detectar√° el movimiento de la mano en tiempo real a trav√©s de la c√°mara web, identificando cu√°ndo el usuario quiere agarrar, mover y soltar objetos virtuales mediante gestos naturales.

üîß Tecnolog√≠as a utilizar:

Python 3.x
OpenCV para captura de video y procesamiento de imagen
MediaPipe para detecci√≥n y seguimiento de manos
Tkinter o interfaz gr√°fica simple (opcional)
Git + GitHub para control de versiones
üìã Objetivos del Proyecto:

Implementar un sistema de seguimiento de manos en tiempo real.
Detectar gestos de ‚Äúagarre‚Äù (como juntar dedos √≠ndice y pulgar).
Permitir que el usuario mueva un objeto virtual con su mano.
Soltar el objeto con otro gesto (como separar los dedos).
Visualizar los objetos y gestos en pantalla.
Versionar el proyecto correctamente en GitHub.
Generar un video demostrativo del funcionamiento del proyecto.


------------------------------------------------------------------------------------------------------------
1. Importaciones
      import cv2
      import mediapipe as mp
      import numpy as np
      from dataclasses import dataclass
      from typing import Tuple, Optional
cv2: OpenCV para captura de video, procesamiento de im√°genes y renderizado.
mediapipe: Biblioteca para detecci√≥n de manos (mp.solutions.hands) y dibujo de puntos clave (mp.solutions.drawing_utils).
numpy: Para c√°lculos matem√°ticos, como distancias entre puntos.
dataclasses: Simplifica la creaci√≥n de la clase VirtualObject.
typing: Mejora la legibilidad con anotaciones de tipo (Tuple, Optional).

2. Definici√≥n de la Clase VirtualObject
     @dataclass
class VirtualObject:
    image: np.ndarray
    x: int
    y: int
    scale: float = 1.0
    
    def get_scaled_size(self) -> Tuple[int, int]:
        h, w = self.image.shape[:2]
        return int(w * self.scale), int(h * self.scale)
    
    def draw(self, frame: np.ndarray) -> None:
        h, w = frame.shape[:2]
        obj_w, obj_h = self.get_scaled_size()
        
        #Calcular posici√≥n para centrar la imagen
        x1 = max(0, min(self.x - obj_w // 2, w - obj_w))
        y1 = max(0, min(self.y - obj_h // 2, h - obj_h))
        
        # Redimensionar imagen
        resized = cv2.resize(self.image, (obj_w, obj_h), interpolation=cv2.INTER_AREA)
        
        # Crear m√°scara para transparencia si la imagen tiene canal alfa
        if resized.shape[2] == 4:
            mask = resized[:, :, 3] / 255.0
            inv_mask = 1.0 - mask
            
            # Regi√≥n de inter√©s en el frame
            roi = frame[y1:y1+obj_h, x1:x1+obj_w]
            
            # Superponer imagen con transparencia
            for c in range(3):
                roi[:, :, c] = (mask * resized[:, :, c] + inv_mask * roi[:, :, c]).astype(np.uint8)
        else:
            frame[y1:y1+obj_h, x1:x1+obj_w] = resized
   Define una clase para representar objetos virtuales que el usuario puede mover.
Atributos:
image: Imagen del objeto (un array de NumPy, normalmente PNG con canal alfa).
x, y: Coordenadas del centro del objeto.
scale: Escala de la imagen (1.0 = tama√±o original).
M√©todos:
get_scaled_size: Calcula el ancho y alto de la imagen escalada.
draw: Renderiza la imagen en el frame, manejando transparencia y l√≠mites de pantalla.
C√≥mo se cre√≥:

Se us√≥ @dataclass para simplificar la definici√≥n de la clase.
El m√©todo draw implementa renderizado con transparencia, crucial para que el objeto (como un √≠cono o sprite) se vea profesional.
Se a√±adieron restricciones (max, min) para evitar que el objeto salga de la pantalla.
La l√≥gica de transparencia usa el canal alfa (si existe) para combinar la imagen con el fondo.

3. Inicializaci√≥n de MediaPipe
       mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.8,
    min_tracking_confidence=0.8
)

Qu√© hace:

Configura MediaPipe para detectar una sola mano.
Establece umbrales altos (0.8) para detecci√≥n y seguimiento, garantizando precisi√≥n.
Se limit√≥ a una mano (max_num_hands=1) para simplificar la interacci√≥n.
Los umbrales se ajustaron tras pruebas para equilibrar sensibilidad y estabilida

4. Carga de la Imagen del Objeto
       try:
    obj_image = cv2.imread("object.png", cv2.IMREAD_UNCHANGED)
    if obj_image is None:
        raise FileNotFoundError("No se encontr√≥ object.png")
except Exception as e:
    print(f"Error al cargar la imagen: {e}")
    # Fallback a un rect√°ngulo si falla la carga
    obj_image = np.zeros((100, 100, 4), dtype=np.uint8)
    obj_image[:, :, :3] = (0, 255, 0)  # Verde
    obj_image[:, :, 3] = 255  # Opaco

   Intenta cargar object.png con soporte para canal alfa (IMREAD_UNCHANGED).
Si falla, crea una imagen de respaldo (cuadrado verde opaco).
C√≥mo se cre√≥:

Se us√≥ un bloque try-except para manejar errores (archivo no encontrado, formato inv√°lido).
El fallback asegura que la aplicaci√≥n no se bloquee si falta la imagen.

6. Configuraci√≥n de la Captura de Video
   cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("Error: No se pudo abrir la c√°mara")
    exit()
Inicia la captura de video desde la c√°mara predeterminada (√≠ndice 0).
Verifica si la c√°mara est√° disponible; si no, termina el programa.
C√≥mo se cre√≥:

Se a√±adi√≥ manejo de errores para evitar fallos si la c√°mara no funciona.
El √≠ndice 0 es est√°ndar para la c√°mara web integrada.

7. Variables de Estado
   holding = False
smooth_factor = 0.3  # Para suavizar el movimiento


holding: Indica si el usuario est√° "agarrando" el objeto (pellizco activo).
smooth_factor: Controla la suavidad del movimiento (0.3 = 30% hacia la nueva posici√≥n por frame).
C√≥mo se cre√≥:

smooth_factor se ajust√≥ experimentalmente para un movimiento fluido sin retraso notable.
holding es un simple booleano para rastrear el estado del gesto.

8. Funci√≥n Auxiliar
   def calculate_distance(p1: Tuple[int, int], p2: Tuple[int, int]) -> float:
    return np.linalg.norm(np.array(p1) - np.array(p2))

   
Calcula la distancia euclidiana entre dos puntos (usada para detectar el pellizco).
C√≥mo se cre√≥:

Se extrajo a una funci√≥n para reutilizaci√≥n y claridad.
Usa NumPy para un c√°lculo eficiente.

9. Bucle Principal
    while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

   Lee frames de la c√°mara en un bucle continuo.
Si no se puede leer un frame, sale del bucle.
C√≥mo se cre√≥:

Estructura est√°ndar para procesamiento de video en tiempo real con OpenCV.

10. Preprocesamiento del Frame
    frame = cv2.flip(frame, 1)
rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
results = hands.process(rgb_frame)

nvierte el frame horizontalmente (espejo) para una experiencia m√°s natural.
Convierte el frame de BGR (formato de OpenCV) a RGB (requerido por MediaPipe).
Procesa el frame con MediaPipe para detectar manos.
C√≥mo se cre√≥:

El flip es un est√°ndar en aplicaciones de seguimiento para que los movimientos sean intuitivos.
La conversi√≥n de color es necesaria porque MediaPipe espera RGB.

11. Procesamiento de la Detecci√≥n de Manos
    if results.multi_hand_landmarks:
    for hand_landmarks in results.multi_hand_landmarks:
        h, w, _ = frame.shape
        
        # Puntos clave
        index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]
        thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]
        
        # Convertir a p√≠xeles
        x_index = int(index_tip.x * w)
        y_index = int(index_tip.y * h)
        x_thumb = int(thumb_tip.x * w)
        y_thumb = int(thumb_tip.y * h)
    Si se detectan manos, itera sobre las manos encontradas (aunque est√° limitado a una).
Obtiene las coordenadas normalizadas (0 a 1) de la punta del √≠ndice y el pulgar.
Convierte estas coordenadas a p√≠xeles usando las dimensiones del frame.
C√≥mo se cre√≥:

Se seleccionaron INDEX_FINGER_TIP y THUMB_TIP porque son ideales para detectar un pellizco.
La conversi√≥n a p√≠xeles es directa: multiplica las coordenadas normalizadas por el ancho (w) y alto (h).


12. Dibujo de la Mano
    mp_draw.draw_landmarks(
    frame,
    hand_landmarks,
    mp_hands.HAND_CONNECTIONS,
    mp_draw.DrawingSpec(color=(0, 255, 255), thickness=2),
    mp_draw.DrawingSpec(color=(255, 0, 255), thickness=4)
)

Dibuja los puntos clave y conexiones de la mano en el frame.
Usa colores personalizados: cian (0, 255, 255) para conexiones, magenta (255, 0, 255) para puntos.
C√≥mo se cre√≥:

Se personalizaron los colores y grosores para un look m√°s atractivo.
DrawingSpec permite ajustar el estilo visual de MediaPipe.

13. Detecci√≥n del Gesto de Agarre
    distance = calculate_distance((x_index, y_index), (x_thumb, y_thumb))
holding = distance < 40

Calcula la distancia entre el √≠ndice y el pulgar.
Si es menor a 40 p√≠xeles, activa el estado holding.
C√≥mo se cre√≥:

El umbral de 40 p√≠xeles se ajust√≥ tras pruebas para detectar pellizcos de forma confiable.
La funci√≥n calculate_distance asegura un c√°lculo preciso.

14. Efectos Visuales al Agarrar
    if holding:
    cv2.circle(frame, (x_index, y_index), 15, (0, 255, 0), 2)
    virtual_obj.scale = min(virtual_obj.scale + 0.02, 0.7)
else:
    virtual_obj.scale = max(virtual_obj.scale - 0.02, 0.5)

    Si holding es verdadero:
Dibuja un c√≠rculo verde en la punta del √≠ndice como retroalimentaci√≥n visual.
Aumenta la escala del objeto hasta un m√°ximo de 0.7.
Si no, reduce la escala hasta un m√≠nimo de 0.5.
C√≥mo se cre√≥:

El c√≠rculo se a√±adi√≥ para confirmar visualmente el agarre.
La animaci√≥n de escala (0.02 por frame) crea un efecto din√°mico sin ser abrumador.
Los l√≠mites (0.5, 0.7) evitan que el objeto sea demasiado peque√±o o grande.

15. Movimiento del Objeto
    if holding:
    target_x, target_y = x_index, y_index
    virtual_obj.x = int(virtual_obj.x * (1 - smooth_factor) + target_x * smooth_factor)
    virtual_obj.y = int(virtual_obj.y * (1 - smooth_factor) + target_y * smooth_factor)

Si el usuario est√° agarrando, actualiza la posici√≥n del objeto.
Usa interpolaci√≥n lineal para suavizar el movimiento: combina la posici√≥n actual con la nueva usando smooth_factor.
C√≥mo se cre√≥:

La interpolaci√≥n (smooth_factor = 0.3) asegura movimientos fluidos, evitando saltos bruscos.
Las coordenadas objetivo son las del √≠ndice, ya que es el punto principal del gesto.

16. Texto Informativo
  cv2.putText(frame, "Presiona 'q' para salir", (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

Muestra un mensaje en la esquina superior izquierda para indicar c√≥mo salir.
C√≥mo se cre√≥:

Se us√≥ una fuente legible (HERSHEY_SIMPLEX) con tama√±o y grosor moderados.
El color blanco es visible en la mayor√≠a de fondos.
    
